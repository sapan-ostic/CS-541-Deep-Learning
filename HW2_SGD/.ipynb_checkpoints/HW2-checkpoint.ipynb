{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# just to overwrite default colab style\n",
    "plt.style.use('default')\n",
    "plt.style.use('seaborn-talk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent with $L_2$ Regularization \n",
    "$\\hat{y} = xw + b$\n",
    "\n",
    "$ \\theta = \\begin{bmatrix} \n",
    "          w  \\\\\n",
    "          b \n",
    "     \\end{bmatrix} $\n",
    "\n",
    "$X = \\begin{bmatrix} \n",
    "          x_1 & 1\\\\ \n",
    "          x_2 & 1\\\\\n",
    "          \\cdots & \\cdots \\\\\n",
    "          x_n & 1 \n",
    "     \\end{bmatrix} $\n",
    "\n",
    "$ \\hat{y} = x \\theta $  \n",
    "\n",
    "$J(\\theta) = \\frac{1}{2n} \\sum ( \\hat{y}_{test} - y_{test} )^2 + \\alpha w^T w$ \n",
    "\n",
    "Note that, in regularization we only penalize the weights and not the bias. \n",
    "\n",
    "$J(\\theta) = \\frac{1}{2n} \\sum ( x_i \\theta - y_{test} )^2 + \\alpha w^T w$\n",
    "\n",
    "$\\nabla_{\\theta} J(\\theta) = -\\frac{1}{n} X^T.[X \\theta - Y_{test} ]$\n",
    "\n",
    "$\\theta = \\theta - \\epsilon \\nabla_{\\theta} J(\\theta)$\n",
    "\n",
    "$\\theta = \\theta + \\frac{\\epsilon}{n} X^T.[X \\theta - Y_{test}]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression using Stochastic Gradient Descent\n",
    "def batchloader(X, Y, batchsize = 20):\n",
    "    n = Y.shape[0]\n",
    "    idx = np.random.choice(np.arange(n),size=batchsize,replace=False)\n",
    "    X_batch = X[idx,:]\n",
    "    Y_batch = Y[idx,:]\n",
    "    return X_batch, Y_batch\n",
    "\n",
    "def computeRegularizedCost(X, Y, theta, w, alpha=0.2):\n",
    "    fMSE = 0\n",
    "    n = Y.shape[0]\n",
    "    fMSE = (1/(2*n))*( np.dot( (np.dot(X,theta) - Y).transpose(), np.dot(X,theta) - Y)) + alpha*np.dot(w.T, w) \n",
    "    return fMSE \n",
    "\n",
    "def computeMSE(X,Y,theta):\n",
    "    fMSE = 0\n",
    "    n = Y.shape[0]\n",
    "    fMSE = (1/(2*n))*( np.dot( (np.dot(X,theta) - Y).transpose(), np.dot(X,theta) - Y))\n",
    "    return fMSE\n",
    "\n",
    "def SGD(X, Y, learning_rate=0.001, epochs=100, batchsize = 20, alpha = 0.2):\n",
    "    \n",
    "    n, m = X.shape\n",
    "    n, p = Y.shape\n",
    "    \n",
    "    w = np.random.randn(m,p)\n",
    "    b = np.random.randn(p,p)\n",
    "    \n",
    "    # preprocessing Data\n",
    "    X = np.append(X, np.ones((n,p)), axis=1)\n",
    "    \n",
    "    n,m_new = X.shape\n",
    "\n",
    "    COST = np.zeros(epochs)\n",
    "    THETAs = np.zeros((m_new, p, epochs))\n",
    "    \n",
    "    theta = np.append(w, b, axis=0)\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        THETAs[:,:,i] = theta\n",
    "    \n",
    "        # Get Batch \n",
    "        X_batch, Y_batch = batchloader(X, Y, batchsize) \n",
    "        Y_pred = np.dot(X_batch,theta)\n",
    "        \n",
    "        # perform gradient descent\n",
    "        gradJ = (1/n)*( np.dot(X_batch.T, (Y_pred - Y_batch) )) \n",
    "        theta = theta - learning_rate * gradJ - 2*alpha*np.append(w,np.zeros((p,p)),axis=0)\n",
    "         \n",
    "        w = theta[:m,:p]    \n",
    "\n",
    "        # get cost\n",
    "        COST[i] = computeMSE(X_batch, Y_batch, theta)\n",
    "    \n",
    "    return theta, THETAs, COST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-27-3a61da1c1c0c>, line 29)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-27-3a61da1c1c0c>\"\u001b[0;36m, line \u001b[0;32m29\u001b[0m\n\u001b[0;31m    BATCHSIZE = [0.1 0.2 0.3 0.5]\u001b[0m\n\u001b[0m                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "X = np.load(\"age_regression_Xtr.npy\")\n",
    "n = X.shape[0]\n",
    "X = X.reshape((n,-1))\n",
    "Y = np.load(\"age_regression_ytr.npy\")\n",
    "Y = Y.reshape(-1,1)\n",
    "\n",
    "# Get Validation Set\n",
    "idx = np.arange(n)\n",
    "validation_set_size = round(0.2*n) # 20%\n",
    "idx_v = np.random.choice(idx,size=validation_set_size,replace=False)\n",
    "Xv = X[idx_v,:]\n",
    "Yv = Y[idx_v,:]\n",
    "\n",
    "# Testing \n",
    "n_v = Xv.shape[0]\n",
    "Xv = Xv.reshape((n_v,-1))\n",
    "\n",
    "# Get Training Data Set\n",
    "idx_tr = np.setxor1d(idx, idx_v)\n",
    "X_tr = X[idx_tr,:]\n",
    "Y_tr = Y[idx_tr,:]\n",
    "print(X_tr.shape)\n",
    "# batchsize = round(0.2*X_tr.shape[0]) # 20% of training set\n",
    "\n",
    "# Tune Hyper parameter\n",
    "LR = [0.001, 0.005, 0.01, 0.05]\n",
    "EPOCHS = [50, 100, 200, 400]\n",
    "BATCHSIZE = [0.1 0.2 0.3 0.5]\n",
    "ALPHA = [0.001 0.002 0.005 0.01]\n",
    "\n",
    "cost = 10000\n",
    "for lr_ in LR:\n",
    "    for epochs_ in EPOCHS:\n",
    "        for bs_ in BATCHSIZE:\n",
    "            for alpha_ in ALPHA:\n",
    "                theta, THETAs, COST = SGD(X_tr, Y_tr, learning_rate=lr_, epochs=epochs_, batchsize=bs_, alpha=alpha_)\n",
    "                \n",
    "                if(COST[-1]<cost):\n",
    "                    lr = lr_\n",
    "                    epochs = epochs_\n",
    "                    bs = bs_\n",
    "                    alpha = alpha_\n",
    "                \n",
    "\n",
    "plt.plot(COST)\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('Stochastic Gradient Descent with L2 Regularization')\n",
    "plt.show()\n",
    "\n",
    "# Testing \n",
    "X_te = np.load(\"age_regression_Xte.npy\")\n",
    "n = X_te.shape[0]\n",
    "X_te = X_te.reshape((n,-1))\n",
    "\n",
    "# preprocessing Data\n",
    "X_te = np.append(X_te, np.ones((n,1)), axis=1)\n",
    "\n",
    "yte = np.load(\"age_regression_yte.npy\")\n",
    "yte = yte.reshape(-1,1)\n",
    "MSE_test = computeMSE(X_te, yte, theta)\n",
    "print('MSE on test data: ', MSE_test[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
