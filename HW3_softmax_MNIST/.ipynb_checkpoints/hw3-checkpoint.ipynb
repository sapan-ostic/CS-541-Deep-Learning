{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# just to overwrite default colab style\n",
    "plt.style.use('default')\n",
    "plt.style.use('seaborn-talk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression using Stochastic Gradient Descent\n",
    "def batchloader(X, Y, batchsize = 20):\n",
    "    n = Y.shape[0]\n",
    "    idx = np.random.choice(np.arange(n),size=batchsize,replace=False)\n",
    "    X_batch = X[idx,:]\n",
    "    Y_batch = Y[idx,:]\n",
    "    return X_batch, Y_batch\n",
    "\n",
    "def softmax(X):\n",
    "    exps = np.exp(X)\n",
    "    return exps / np.sum(exps)\n",
    "\n",
    "def crossEntropy(X, Y, theta):\n",
    "    fce = 0\n",
    "    \n",
    "    n, m = X.shape\n",
    "    n, p = Y.shape \n",
    "    \n",
    "    for i in range(n):\n",
    "        Y_pred = softmax(np.dot(X[i],theta))\n",
    "        fce += np.dot(Y[i].T, np.log(Y_pred))  \n",
    "    fce *= (-1/n)\n",
    "#     print(fce)\n",
    "    return fce\n",
    "    \n",
    "def SGD(X, Y, learning_rate=0.001, epochs=100, bs = 0.2, alpha = 0.2):\n",
    "    \n",
    "    n, m = X.shape\n",
    "    n, p = Y.shape\n",
    "    \n",
    "    w = np.random.randn(m,p)\n",
    "    b = np.random.randn(1,p)\n",
    "    \n",
    "    batchsize = round(bs*n)\n",
    "    \n",
    "    # preprocessing Data\n",
    "    X = np.append(X, np.ones((n,1)), axis=1)\n",
    "    \n",
    "    n, mpn = X.shape\n",
    "\n",
    "    COST = np.zeros(epochs)\n",
    "    ACC = np.zeros(epochs)\n",
    "    theta = np.append(w, b, axis=0)\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        alpha *= 0.99\n",
    "        # Get Batch \n",
    "        X_batch, Y_batch = batchloader(X, Y, batchsize) \n",
    "        Y_pred = softmax(np.dot(X_batch,theta))\n",
    "        \n",
    "        # perform gradient descent\n",
    "        gradJ = (1/n)*( np.dot(X_batch.T, (Y_pred - Y_batch) )) \n",
    "        theta = theta - learning_rate * gradJ - 2*alpha*np.append(w,np.zeros((1,p)),axis=0)\n",
    "         \n",
    "        w = theta[:m,:p]    \n",
    "\n",
    "        # get cost\n",
    "        COST[i] = crossEntropy(X_batch, Y_batch, theta)\n",
    "        acc = accuracy(Y_pred, Y_batch)\n",
    "        ACC[i] = acc\n",
    "        print(\"acc:\", acc)\n",
    "    return theta, COST, ACC \n",
    "    \n",
    "def accuracy(Y_pred, Y):\n",
    "    n,p = Y.shape\n",
    "    acc = np.sum([np.argmax(Y_pred[i])==np.argmax(Y[i]) for i in range(n)])/(0.01*n)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.96\n",
      "iter:  0 , loss:  11.52473838607429\n",
      "11.2\n",
      "iter:  1 , loss:  11.082956664825186\n",
      "6.58\n",
      "iter:  2 , loss:  9.869023394071686\n",
      "17.28\n",
      "iter:  3 , loss:  5.435014731350501\n",
      "15.42\n",
      "iter:  4 , loss:  11.26120916546845\n",
      "10.88\n",
      "iter:  5 , loss:  9.016304011025214\n",
      "6.2\n",
      "iter:  6 , loss:  9.130103531862238\n",
      "11.34\n",
      "iter:  7 , loss:  5.7428510294623365\n",
      "7.14\n",
      "iter:  8 , loss:  14.782858586671411\n",
      "6.7\n",
      "iter:  9 , loss:  12.329391890873936\n",
      "6.5\n",
      "iter:  10 , loss:  7.929299528993527\n",
      "13.12\n",
      "iter:  11 , loss:  6.053432106652638\n",
      "8.92\n",
      "iter:  12 , loss:  14.897557445207749\n",
      "14.02\n",
      "iter:  13 , loss:  11.209017016608671\n",
      "8.84\n",
      "iter:  14 , loss:  8.611335422424986\n",
      "12.54\n",
      "iter:  15 , loss:  6.852292442695873\n",
      "8.4\n",
      "iter:  16 , loss:  14.375539295701412\n",
      "9.18\n",
      "iter:  17 , loss:  10.748271879871918\n",
      "5.18\n",
      "iter:  18 , loss:  11.077312872420881\n",
      "10.74\n",
      "iter:  19 , loss:  5.48774852457642\n",
      "10.26\n",
      "iter:  20 , loss:  14.141961733532023\n",
      "11.06\n",
      "iter:  21 , loss:  14.499004298124444\n",
      "11.94\n",
      "iter:  22 , loss:  9.193137736248701\n",
      "12.02\n",
      "iter:  23 , loss:  4.87395779614543\n",
      "9.3\n",
      "iter:  24 , loss:  13.62520758334403\n",
      "6.46\n",
      "iter:  25 , loss:  10.279344333498765\n",
      "12.4\n",
      "iter:  26 , loss:  9.724509907169779\n",
      "11.2\n",
      "iter:  27 , loss:  5.957484287739356\n",
      "11.54\n",
      "iter:  28 , loss:  13.324345485961757\n",
      "14.24\n",
      "iter:  29 , loss:  12.074372990934593\n",
      "11.02\n",
      "iter:  30 , loss:  9.009446465075124\n",
      "13.42\n",
      "iter:  31 , loss:  5.170905229097014\n",
      "11.92\n",
      "iter:  32 , loss:  12.462702051252629\n",
      "11.28\n",
      "iter:  33 , loss:  10.632947742191448\n",
      "15.42\n",
      "iter:  34 , loss:  7.38664313744496\n",
      "9.54\n",
      "iter:  35 , loss:  5.946950238973478\n",
      "10.88\n",
      "iter:  36 , loss:  11.627862402316547\n",
      "12.48\n",
      "iter:  37 , loss:  10.695323114903285\n",
      "11.42\n",
      "iter:  38 , loss:  7.2167702276276975\n",
      "8.3\n",
      "iter:  39 , loss:  6.433028448891647\n",
      "7.06\n",
      "iter:  40 , loss:  13.098293606390689\n",
      "7.58\n",
      "iter:  41 , loss:  12.390085619163349\n",
      "9.78\n",
      "iter:  42 , loss:  9.208546629525596\n",
      "11.96\n",
      "iter:  43 , loss:  4.6965160461375834\n",
      "10.38\n",
      "iter:  44 , loss:  11.36493389179258\n",
      "13.26\n",
      "iter:  45 , loss:  8.806390137180896\n",
      "7.94\n",
      "iter:  46 , loss:  8.235075151893142\n",
      "8.62\n",
      "iter:  47 , loss:  4.850739085523891\n",
      "8.7\n",
      "iter:  48 , loss:  10.785812736596526\n",
      "13.14\n",
      "iter:  49 , loss:  10.31066087227001\n",
      "6.38\n",
      "iter:  50 , loss:  8.885720851523761\n",
      "6.7\n",
      "iter:  51 , loss:  5.909111914883379\n",
      "6.66\n",
      "iter:  52 , loss:  13.912022001809758\n",
      "8.66\n",
      "iter:  53 , loss:  11.859338344947865\n",
      "15.44\n",
      "iter:  54 , loss:  7.365963202579962\n",
      "10.36\n",
      "iter:  55 , loss:  5.230493871572537\n",
      "15.36\n",
      "iter:  56 , loss:  9.315158419159571\n",
      "10.24\n",
      "iter:  57 , loss:  11.294460775125684\n",
      "7.84\n",
      "iter:  58 , loss:  8.344338865885154\n",
      "9.3\n",
      "iter:  59 , loss:  6.206888570317509\n",
      "12.54\n",
      "iter:  60 , loss:  13.387633295860605\n",
      "14.1\n",
      "iter:  61 , loss:  10.981763011150454\n",
      "19.02\n",
      "iter:  62 , loss:  7.0745513873707635\n",
      "19.62\n",
      "iter:  63 , loss:  4.281200409377174\n",
      "learning rate:  0.05 , Epochs:  50 , mini-batchsize (in %):  50.0 , alpha:  0.01\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "Xtr = np.load(\"mnist_train_images.npy\")\n",
    "n = Xtr.shape[0]\n",
    "Xtr = Xtr.reshape((n,-1))\n",
    "Ytr = np.load(\"mnist_train_labels.npy\")\n",
    "\n",
    "# Get Validation Set\n",
    "Xv = np.load(\"mnist_validation_images.npy\")\n",
    "nv = Xv.shape[0]\n",
    "Xv = Xv.reshape((nv,-1)) # feature vector is row vector \n",
    "Yv = np.load(\"mnist_validation_labels.npy\")\n",
    "\n",
    "# preprocessing on validation set \n",
    "Xv = np.append(Xv, np.ones((nv,1)), axis=1)\n",
    "\n",
    "# print(Xtr.shape, Ytr.shape, Xv.shape, Yv.shape)\n",
    "\n",
    "# Tune Hyper parameter\n",
    "LR = [0.001, 0.005, 0.01, 0.05]\n",
    "EPOCHS = [50, 50, 50, 50]\n",
    "BATCHSIZE = [0.1, 0.2, 0.3, 0.5]\n",
    "ALPHA = [0.001, 0.002, 0.005, 0.01]\n",
    "\n",
    "cost = 1000000\n",
    "\n",
    "iter = 0\n",
    "\n",
    "for lr_ in LR:\n",
    "#     for epochs_ in EPOCHS:\n",
    "    for bs_ in BATCHSIZE:\n",
    "        for alpha_ in ALPHA:\n",
    "            theta, COST, ACC = SGD(Xtr, Ytr, learning_rate=lr_, epochs=epochs_, bs=bs_, alpha=alpha_) \n",
    "\n",
    "            Y_pred = softmax(np.dot(Xv, theta))\n",
    "            accu = accuracy(Y_pred, Yv)\n",
    "            print(accu)\n",
    "            loss = COST[-1]\n",
    "            print(\"iter: \", iter, \", loss: \", COST[-1])\n",
    "            iter += 1 \n",
    "\n",
    "            if(loss<cost):\n",
    "                lr = lr_\n",
    "                epochs = epochs_\n",
    "                bs = bs_\n",
    "                alpha = alpha_\n",
    "                cost=loss\n",
    "                    \n",
    "print('learning rate: ', lr, ', Epochs: ', epochs, ', mini-batchsize (in %): ', bs*100, ', alpha: ', alpha)                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 11.254545454545454\n",
      "acc: 13.090909090909092\n",
      "acc: 15.10909090909091\n",
      "acc: 18.10909090909091\n",
      "acc: 22.745454545454546\n",
      "acc: 25.472727272727273\n",
      "acc: 28.272727272727273\n",
      "acc: 30.70909090909091\n",
      "acc: 32.4\n",
      "acc: 35.236363636363635\n",
      "acc: 38.78181818181818\n",
      "acc: 41.25454545454546\n",
      "acc: 43.07272727272727\n",
      "acc: 45.78181818181818\n",
      "acc: 47.29090909090909\n",
      "acc: 48.61818181818182\n",
      "acc: 50.8\n",
      "acc: 51.29090909090909\n",
      "acc: 50.654545454545456\n",
      "acc: 52.69090909090909\n",
      "acc: 52.85454545454545\n",
      "acc: 54.14545454545455\n",
      "acc: 53.74545454545454\n",
      "acc: 54.054545454545455\n",
      "acc: 55.21818181818182\n",
      "acc: 56.56363636363636\n",
      "acc: 56.2\n",
      "acc: 56.58181818181818\n",
      "acc: 57.63636363636363\n",
      "acc: 58.836363636363636\n",
      "acc: 58.654545454545456\n",
      "acc: 58.03636363636364\n",
      "acc: 57.92727272727273\n",
      "acc: 58.4\n",
      "acc: 59.2\n",
      "acc: 59.18181818181818\n",
      "acc: 58.72727272727273\n",
      "acc: 58.981818181818184\n",
      "acc: 59.236363636363635\n",
      "acc: 59.2\n",
      "acc: 59.21818181818182\n",
      "acc: 60.0\n",
      "acc: 60.63636363636363\n",
      "acc: 61.127272727272725\n",
      "acc: 60.56363636363636\n",
      "acc: 60.654545454545456\n",
      "acc: 61.58181818181818\n",
      "acc: 61.4\n",
      "acc: 62.45454545454545\n",
      "acc: 63.45454545454545\n",
      "acc: 62.2\n",
      "acc: 62.127272727272725\n",
      "acc: 61.018181818181816\n",
      "acc: 62.56363636363636\n",
      "acc: 62.2\n",
      "acc: 62.43636363636364\n",
      "acc: 63.49090909090909\n",
      "acc: 62.0\n",
      "acc: 63.50909090909091\n",
      "acc: 64.18181818181819\n",
      "acc: 63.07272727272727\n",
      "acc: 64.34545454545454\n",
      "acc: 64.54545454545455\n",
      "acc: 64.50909090909092\n",
      "acc: 64.9090909090909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sapanostic/.local/lib/python3.5/site-packages/ipykernel_launcher.py:10: RuntimeWarning: overflow encountered in exp\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/home/sapanostic/.local/lib/python3.5/site-packages/ipykernel_launcher.py:11: RuntimeWarning: invalid value encountered in true_divide\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/home/sapanostic/.local/lib/python3.5/site-packages/ipykernel_launcher.py:21: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 9.672727272727272\n",
      "acc: 9.690909090909091\n",
      "acc: 10.145454545454545\n",
      "acc: 9.763636363636364\n",
      "acc: 9.345454545454546\n",
      "acc: 10.145454545454545\n",
      "acc: 9.527272727272727\n",
      "acc: 10.018181818181818\n",
      "acc: 9.49090909090909\n",
      "acc: 9.690909090909091\n",
      "acc: 9.581818181818182\n",
      "acc: 10.163636363636364\n",
      "acc: 9.745454545454546\n",
      "acc: 9.436363636363636\n",
      "acc: 9.945454545454545\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-179-682c4c023e73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mbs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Training on tuned hyperparameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCOST\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mACC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYtr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mACC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-166-7f51bdc50692>\u001b[0m in \u001b[0;36mSGD\u001b[0;34m(X, Y, learning_rate, epochs, bs, alpha)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;31m# get cost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mCOST\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrossEntropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mACC\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-166-7f51bdc50692>\u001b[0m in \u001b[0;36mcrossEntropy\u001b[0;34m(X, Y, theta)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mY_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mfce\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mfce\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m#     print(fce)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "lr = 10\n",
    "alpha = 0.001\n",
    "bs = 0.1\n",
    "# Training on tuned hyperparameters\n",
    "theta, COST, ACC = SGD(Xtr, Ytr, learning_rate=lr, epochs=epochs, bs=bs, alpha=alpha)\n",
    "\n",
    "plt.plot(ACC)\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('Stochastic Gradient Descent with L2 Regularization')\n",
    "plt.show()\n",
    "\n",
    "# Testing \n",
    "X_te = np.load(\"mnist_test_images.npy\")\n",
    "n = X_te.shape[0]\n",
    "X_te = X_te.reshape((n,-1))\n",
    "\n",
    "# preprocessing Data\n",
    "X_te = np.append(X_te, np.ones((n,1)), axis=1)\n",
    "yte = np.load(\"mnist_test_labels.npy\")\n",
    "\n",
    "MSE_test = crossEntropy(X_te, yte, theta)\n",
    "Y_pred = softmax(np.dot(X_te, theta))\n",
    "accu = accuracy(Y_pred, yte)\n",
    "\n",
    "print('accuracy', accu)\n",
    "print('MSE on test data: ', MSE_test)\n",
    "print('Tuned Hyperparameters: ')\n",
    "print('learning rate: ', lr, ', Epochs: ', epochs, ', mini-batchsize (in %): ', bs*100, ', alpha: ', alpha)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
