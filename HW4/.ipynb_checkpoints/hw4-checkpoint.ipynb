{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import configparser\n",
    "\n",
    "# just to overwrite default colab style\n",
    "plt.style.use('default')\n",
    "plt.style.use('seaborn-talk')\n",
    "\n",
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('config.config')\n",
    "default = 'DEFAULT'\n",
    "config = config[default]\n",
    "alpha = config['alpha']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activations\n",
    "def relu(X):\n",
    "    return np.maximum(0,X)\n",
    "\n",
    "def softmax(X):\n",
    "    exp_x = np.exp(X)\n",
    "    probs = exp_x / np.sum(exp_x, axis=1, keepdims=True) # [N x K]\n",
    "    return probs\n",
    "\n",
    "def batchloader(X, Y, batchsize = 32):\n",
    "    n = Y.shape[0]\n",
    "    idx = np.random.choice(np.arange(n),size=batchsize,replace=False)\n",
    "    X_batch = X[idx,:]\n",
    "    Y_batch = Y[idx,:]\n",
    "    \n",
    "    return X_batch, Y_batch\n",
    "\n",
    "def crossEntropy(X, Y, theta):\n",
    "    n, m = X.shape\n",
    "    n, p = Y.shape \n",
    "    fce = 0\n",
    "    \n",
    "    for i in range(n):\n",
    "        Y_pred = softmax(np.dot(X[i],theta))\n",
    "        fce += np.dot(Y[i].T, np.log(Y_pred))  \n",
    "    fce *= (-1/n)\n",
    "    return fce\n",
    "\n",
    "# Storing all weights ndarray as list\n",
    "def get_weights(layers):\n",
    "    weights = []\n",
    "    Nlayers = len(layers)\n",
    "\n",
    "    for i in range(Nlayers - 1):\n",
    "        w = np.random.rand(layers[i], layers[i+1])-1 # 1 added for the bias\n",
    "        weights.append(w)    \n",
    "    return weights\n",
    "\n",
    "def forward(X, W):\n",
    "    h = X  #h0 = X\n",
    "    network = [X]\n",
    "    \n",
    "    for w in W[:-1]:   # in => h1 => out \n",
    "        z = np.dot(h, w) # next layer\n",
    "        h = relu(z)      # ReLU activation\n",
    "        network.append(h)    \n",
    "    y_pred = softmax(np.dot(h, W[-1])) # output with softmax\n",
    "    network.append(y_pred)\n",
    "    return network # h1, h2, ... y_hat\n",
    "\n",
    "def accuracy(Y_pred, Y):\n",
    "    n,p = Y.shape\n",
    "    acc = np.sum([np.argmax(Y_pred[i])==np.argmax(Y[i]) for i in range(n)])/(0.01*n)\n",
    "    return acc\n",
    "\n",
    "def SGD(X, Y, learning_rate=0.1, epochs=100, bs = 32, alpha = 0.002):\n",
    "\n",
    "    n, m = X.shape\n",
    "    n, p = Y.shape\n",
    "    \n",
    "    # Define layers: Input Hidden Output\n",
    "    # Enter no. of perceptrons in each layer\n",
    "    layers = [m] + [256] + [p]\n",
    "    \n",
    "    # Initialize Weights\n",
    "    weights = get_weights(layers)\n",
    "\n",
    "    COST = np.zeros(epochs)\n",
    "    ACC = np.zeros(epochs)\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        \n",
    "        # Get Batch \n",
    "        X_batch, Y_batch = batchloader(X, Y, bs) \n",
    "        \n",
    "        # Forward Pass\n",
    "        network = forward(X_batch,weights)\n",
    "\n",
    "        # Back Propagation\n",
    "        weights = backprop(network, Y_batch, weights, learning_rate)    \n",
    "\n",
    "        # get cost\n",
    "        COST[i] = crossEntropy(X_batch, Y_batch, theta)\n",
    "        acc = accuracy(Y_pred, Y_batch)\n",
    "        ACC[i] = acc\n",
    "        print(\"acc:\", acc)\n",
    "        \n",
    "    return theta, COST, ACC \n",
    "\n",
    "def backprop(network, y, W, lr=0.01):\n",
    "    y_pred = network[-1]\n",
    "    print('y:', y.shape)\n",
    "    dJ_dz = y - y_pred # Initial dJ_dz (deltas)\n",
    "    \n",
    "    h = network[-1] # y_hat\n",
    "    N = range(len(network))\n",
    "    dW = []\n",
    "\n",
    "    for i in list(reversed(N)):\n",
    "        \n",
    "        print('i',i)\n",
    "        print(list(reversed(N)))\n",
    "        print('dJ_dz:', dJ_dz.shape)\n",
    "        print('network[i-1]',network[i-1].shape)\n",
    "        dJ_dW = np.dot(dJ_dz.T, network[i-1]) # dJ_dw1 = dJ_dz * dz_dw\n",
    "        \n",
    "        dW.append(dJ_dW)\n",
    "\n",
    "        dz_dh = W[i-1]\n",
    "        \n",
    "        z_im1 = np.dot(network[i-2], W[i-2]) \n",
    "        \n",
    "        dh_dz = np.asarray(z_im1 >= 0, dtype=int)\n",
    "        \n",
    "        print('dJ_dW:', dJ_dW.shape)\n",
    "        print('dJ_dz:', dJ_dz.shape)\n",
    "        print('dz_dh:',dz_dh.shape)\n",
    "        print('dh_dz:',dh_dz.shape)\n",
    "        \n",
    "        \n",
    "        print('z_im1', z_im1.shape)\n",
    "        print('h1:', network[i-1].shape)\n",
    "        print('W2', W[i-1].shape)\n",
    "        \n",
    "        dJ_dz = np.dot(np.dot(dJ_dz,dz_dh.T),dh_dz.T)\n",
    "\n",
    "    for i in N:\n",
    "        W[i] += - lr * dW[i]\n",
    "        \n",
    "    return W     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y: (16, 10)\n",
      "i 2\n",
      "[2, 1, 0]\n",
      "dJ_dz: (16, 10)\n",
      "network[i-1] (16, 256)\n",
      "dJ_dW: (10, 256)\n",
      "dJ_dz: (16, 10)\n",
      "dz_dh: (256, 10)\n",
      "dh_dz: (16, 256)\n",
      "z_im1 (16, 256)\n",
      "h1: (16, 256)\n",
      "W2 (256, 10)\n",
      "i 1\n",
      "[2, 1, 0]\n",
      "dJ_dz: (16, 16)\n",
      "network[i-1] (16, 785)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (16,10) and (256,10) not aligned: 10 (dim 1) != 256 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-041d57f58396>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCOST\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mACC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYtr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-49-fed591353600>\u001b[0m in \u001b[0;36mSGD\u001b[0;34m(X, Y, learning_rate, epochs, bs, alpha)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;31m# Back Propagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;31m# get cost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-49-fed591353600>\u001b[0m in \u001b[0;36mbackprop\u001b[0;34m(network, y, W, lr)\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mdz_dh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mz_im1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mdh_dz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_im1\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (16,10) and (256,10) not aligned: 10 (dim 1) != 256 (dim 0)"
     ]
    }
   ],
   "source": [
    "theta, COST, ACC = SGD(Xtr, Ytr, learning_rate=lr_, epochs=epochs_, bs=bs_, alpha=alpha_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y: (16, 10)\n",
      "dJ_dz: (16, 10)\n",
      "dJ_dW: (10, 256)\n",
      "W[i] (785, 256)\n",
      "z_im1 (16, 256)\n",
      "dh_dz: (16, 256)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (16,10) (785,256) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-e217708e0300>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbs_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mBATCHSIZE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0malpha_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mALPHA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                 \u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCOST\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mACC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYtr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                 \u001b[0mY_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-3169ca3c4bbe>\u001b[0m in \u001b[0;36mSGD\u001b[0;34m(X, Y, learning_rate, epochs, bs, alpha)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;31m# Back Propagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;31m# get cost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-3169ca3c4bbe>\u001b[0m in \u001b[0;36mbackprop\u001b[0;34m(network, y, W, lr)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mdh_dz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_im1\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dh_dz:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdh_dz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0mdJ_dz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdJ_dz\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdz_dh\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdh_dz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (16,10) (785,256) "
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "Xtr = np.load(\"mnist_train_images.npy\")\n",
    "n = Xtr.shape[0]\n",
    "Xtr = Xtr.reshape((n,-1))\n",
    "\n",
    "# preprocessing Data\n",
    "Xtr = np.append(Xtr, np.ones((n,1)), axis=1)\n",
    "\n",
    "Ytr = np.load(\"mnist_train_labels.npy\")\n",
    "\n",
    "# Get Validation Set\n",
    "Xv = np.load(\"mnist_validation_images.npy\")\n",
    "nv = Xv.shape[0]\n",
    "Xv = Xv.reshape((nv,-1)) # feature vector is row vector \n",
    "\n",
    "# preprocessing on validation set \n",
    "Xv = np.append(Xv, np.ones((nv,1)), axis=1)\n",
    "\n",
    "Yv = np.load(\"mnist_validation_labels.npy\")\n",
    "\n",
    "# Tune Hyper parameter\n",
    "LR = [0.001, 0.005, 0.01, 0.05]\n",
    "EPOCHS = [50, 50, 50, 50]\n",
    "BATCHSIZE = [16, 32, 128, 256]\n",
    "ALPHA = [0.001, 0.002, 0.005, 0.01]\n",
    "\n",
    "cost = 1000000\n",
    "\n",
    "iter = 0\n",
    "\n",
    "for lr_ in LR:\n",
    "    for epochs_ in EPOCHS:\n",
    "        for bs_ in BATCHSIZE:\n",
    "            for alpha_ in ALPHA:\n",
    "                theta, COST, ACC = SGD(Xtr, Ytr, learning_rate=lr_, epochs=epochs_, bs=bs_, alpha=alpha_) \n",
    "\n",
    "                Y_pred = softmax(np.dot(Xv, theta))\n",
    "                accu = accuracy(Y_pred, Yv)\n",
    "                print(accu)\n",
    "                loss = COST[-1]\n",
    "                print(\"iter: \", iter, \", loss: \", COST[-1])\n",
    "                iter += 1 \n",
    "\n",
    "                if(loss<cost):\n",
    "                    lr = lr_\n",
    "                    epochs = epochs_\n",
    "                    bs = bs_\n",
    "                    alpha = alpha_\n",
    "                    cost=loss\n",
    "                    \n",
    "print('learning rate: ', lr, ', Epochs: ', epochs, ', mini-batchsize (in %): ', bs*100, ', alpha: ', alpha)                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "lr = 10\n",
    "alpha = 0.001\n",
    "bs = 0.1\n",
    "# Training on tuned hyperparameters\n",
    "theta, COST, ACC = SGD(Xtr, Ytr, learning_rate=lr, epochs=epochs, bs=bs, alpha=alpha)\n",
    "\n",
    "plt.plot(ACC)\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('Stochastic Gradient Descent with L2 Regularization')\n",
    "plt.show()\n",
    "\n",
    "# Testing \n",
    "X_te = np.load(\"mnist_test_images.npy\")\n",
    "n = X_te.shape[0]\n",
    "X_te = X_te.reshape((n,-1))\n",
    "\n",
    "# preprocessing Data\n",
    "X_te = np.append(X_te, np.ones((n,1)), axis=1)\n",
    "yte = np.load(\"mnist_test_labels.npy\")\n",
    "\n",
    "MSE_test = crossEntropy(X_te, yte, theta)\n",
    "Y_pred = softmax(np.dot(X_te, theta))\n",
    "accu = accuracy(Y_pred, yte)\n",
    "\n",
    "print('accuracy', accu)\n",
    "print('MSE on test data: ', MSE_test)\n",
    "print('Tuned Hyperparameters: ')\n",
    "print('learning rate: ', lr, ', Epochs: ', epochs, ', mini-batchsize (in %): ', bs*100, ', alpha: ', alpha)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
