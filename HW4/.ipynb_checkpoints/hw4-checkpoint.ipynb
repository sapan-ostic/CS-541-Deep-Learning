{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# just to overwrite default colab style\n",
    "plt.style.use('default')\n",
    "plt.style.use('seaborn-talk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression using Stochastic Gradient Descent\n",
    "def batchloader(X, Y, batchsize = 20):\n",
    "    n = Y.shape[0]\n",
    "    idx = np.random.choice(np.arange(n),size=batchsize,replace=False)\n",
    "    X_batch = X[idx,:]\n",
    "    Y_batch = Y[idx,:]\n",
    "    return X_batch, Y_batch\n",
    "\n",
    "def softmax(X):\n",
    "    exps = np.exp(X)\n",
    "    return exps / np.sum(exps)\n",
    "\n",
    "def crossEntropy(X, Y, theta):\n",
    "    fce = 0\n",
    "    \n",
    "    n, m = X.shape\n",
    "    n, p = Y.shape \n",
    "    \n",
    "    for i in range(n):\n",
    "        Y_pred = softmax(np.dot(X[i],theta))\n",
    "        fce += np.dot(Y[i].T, np.log(Y_pred))  \n",
    "    fce *= (-1/n)\n",
    "#     print(fce)\n",
    "    return fce\n",
    "    \n",
    "def SGD(X, Y, learning_rate=0.001, epochs=100, bs = 0.2, alpha = 0.2):\n",
    "    \n",
    "    n, m = X.shape\n",
    "    n, p = Y.shape\n",
    "    \n",
    "    w = np.random.randn(m,p)\n",
    "    b = np.random.randn(1,p)\n",
    "    \n",
    "    batchsize = round(bs*n)\n",
    "    \n",
    "    # preprocessing Data\n",
    "    X = np.append(X, np.ones((n,1)), axis=1)\n",
    "    \n",
    "    n, mpn = X.shape\n",
    "\n",
    "    COST = np.zeros(epochs)\n",
    "    ACC = np.zeros(epochs)\n",
    "    theta = np.append(w, b, axis=0)\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        alpha *= 0.99\n",
    "        # Get Batch \n",
    "        X_batch, Y_batch = batchloader(X, Y, batchsize) \n",
    "        Y_pred = softmax(np.dot(X_batch,theta))\n",
    "        \n",
    "        # perform gradient descent\n",
    "        gradJ = (1/n)*( np.dot(X_batch.T, (Y_pred - Y_batch) )) \n",
    "        theta = theta - learning_rate * gradJ - 2*alpha*np.append(w,np.zeros((1,p)),axis=0)\n",
    "         \n",
    "        w = theta[:m,:p]    \n",
    "\n",
    "        # get cost\n",
    "        COST[i] = crossEntropy(X_batch, Y_batch, theta)\n",
    "        acc = accuracy(Y_pred, Y_batch)\n",
    "        ACC[i] = acc\n",
    "        print(\"acc:\", acc)\n",
    "    return theta, COST, ACC \n",
    "    \n",
    "def accuracy(Y_pred, Y):\n",
    "    n,p = Y.shape\n",
    "    acc = np.sum([np.argmax(Y_pred[i])==np.argmax(Y[i]) for i in range(n)])/(0.01*n)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "Xtr = np.load(\"mnist_train_images.npy\")\n",
    "n = Xtr.shape[0]\n",
    "Xtr = Xtr.reshape((n,-1))\n",
    "Ytr = np.load(\"mnist_train_labels.npy\")\n",
    "\n",
    "# Get Validation Set\n",
    "Xv = np.load(\"mnist_validation_images.npy\")\n",
    "nv = Xv.shape[0]\n",
    "Xv = Xv.reshape((nv,-1)) # feature vector is row vector \n",
    "Yv = np.load(\"mnist_validation_labels.npy\")\n",
    "\n",
    "# preprocessing on validation set \n",
    "Xv = np.append(Xv, np.ones((nv,1)), axis=1)\n",
    "\n",
    "# print(Xtr.shape, Ytr.shape, Xv.shape, Yv.shape)\n",
    "\n",
    "# Tune Hyper parameter\n",
    "LR = [0.001, 0.005, 0.01, 0.05]\n",
    "EPOCHS = [50, 50, 50, 50]\n",
    "BATCHSIZE = [0.1, 0.2, 0.3, 0.5]\n",
    "ALPHA = [0.001, 0.002, 0.005, 0.01]\n",
    "\n",
    "cost = 1000000\n",
    "\n",
    "iter = 0\n",
    "\n",
    "for lr_ in LR:\n",
    "    for epochs_ in EPOCHS:\n",
    "        for bs_ in BATCHSIZE:\n",
    "            for alpha_ in ALPHA:\n",
    "                theta, COST, ACC = SGD(Xtr, Ytr, learning_rate=lr_, epochs=epochs_, bs=bs_, alpha=alpha_) \n",
    "\n",
    "                Y_pred = softmax(np.dot(Xv, theta))\n",
    "                accu = accuracy(Y_pred, Yv)\n",
    "                print(accu)\n",
    "                loss = COST[-1]\n",
    "                print(\"iter: \", iter, \", loss: \", COST[-1])\n",
    "                iter += 1 \n",
    "\n",
    "                if(loss<cost):\n",
    "                    lr = lr_\n",
    "                    epochs = epochs_\n",
    "                    bs = bs_\n",
    "                    alpha = alpha_\n",
    "                    cost=loss\n",
    "                    \n",
    "print('learning rate: ', lr, ', Epochs: ', epochs, ', mini-batchsize (in %): ', bs*100, ', alpha: ', alpha)                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "lr = 10\n",
    "alpha = 0.001\n",
    "bs = 0.1\n",
    "# Training on tuned hyperparameters\n",
    "theta, COST, ACC = SGD(Xtr, Ytr, learning_rate=lr, epochs=epochs, bs=bs, alpha=alpha)\n",
    "\n",
    "plt.plot(ACC)\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('Stochastic Gradient Descent with L2 Regularization')\n",
    "plt.show()\n",
    "\n",
    "# Testing \n",
    "X_te = np.load(\"mnist_test_images.npy\")\n",
    "n = X_te.shape[0]\n",
    "X_te = X_te.reshape((n,-1))\n",
    "\n",
    "# preprocessing Data\n",
    "X_te = np.append(X_te, np.ones((n,1)), axis=1)\n",
    "yte = np.load(\"mnist_test_labels.npy\")\n",
    "\n",
    "MSE_test = crossEntropy(X_te, yte, theta)\n",
    "Y_pred = softmax(np.dot(X_te, theta))\n",
    "accu = accuracy(Y_pred, yte)\n",
    "\n",
    "print('accuracy', accu)\n",
    "print('MSE on test data: ', MSE_test)\n",
    "print('Tuned Hyperparameters: ')\n",
    "print('learning rate: ', lr, ', Epochs: ', epochs, ', mini-batchsize (in %): ', bs*100, ', alpha: ', alpha)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
